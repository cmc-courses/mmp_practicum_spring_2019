{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# 1 практическое задание. Обучение полносвязной нейронной сети.\n",
    "\n",
    "## Практикум на ЭВМ для 317 группы, весна 2019\n",
    "\n",
    "#### Фамилия, имя: Находнов Максим\n",
    "\n",
    "Дата выдачи: 19 февраля\n",
    "\n",
    "Мягкий дедлайн: 28 февраля 23:59 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Реализация нейронной сети (6 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "В этом задании вы обучите полносвязную нейронную сеть распознавать рукописные цифры (а что же еще, если не их :), [почти] самостоятельно реализовав все составляющие алгоритма обучения и предсказания.\n",
    "\n",
    "Для начала нам понадобится реализовать прямой и обратный проход через слои. Наши слои будут соответствовать следующему интерфейсу (на примере \"тождественного\" слоя):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "class IdentityLayer:\n",
    "    \"\"\"\n",
    "    A building block. Each layer is capable of performing two things:\n",
    "    \n",
    "    - Process input to get output:           \n",
    "    output = layer.forward(input)\n",
    "    \n",
    "    - Propagate gradients through itself:    \n",
    "    grad_input = layer.backward(input, grad_output)\n",
    "    \n",
    "    Some layers also have learnable parameters.\n",
    "    \n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Here you can initialize layer parameters (if any) \n",
    "        and auxiliary stuff. You should enumerate all parameters\n",
    "        in self.params\"\"\"\n",
    "        # An identity layer does nothing\n",
    "        self.input_data = None\n",
    "        self.params = []\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Takes input data of shape [batch, input_units], \n",
    "        returns output data [batch, output_units]\n",
    "        \"\"\"\n",
    "        # An identity layer just returns whatever it gets as input.\n",
    "        self.input_data = input_data\n",
    "        return input_data\n",
    "\n",
    "    def backward(self, grad_output): \n",
    "        \"\"\"\n",
    "        Performs a back propagation step through the layer, \n",
    "        with respect to the given input.\n",
    "        \n",
    "        To compute loss gradients w.r.t input, \n",
    "        you need to apply chain rule (backprop):\n",
    "        \n",
    "        d loss / d input  = (d loss / d layer) *  (d layer / d input)\n",
    "        \n",
    "        Luckily, you already receive d loss / d layer as input, \n",
    "        so you only need to multiply it by d layer / d x.\n",
    "        \n",
    "        The method returns:\n",
    "        * gradient w.r.t input (will be passed to \n",
    "          previous layer's backward method)\n",
    "        * flattened gradient w.r.t. parameters (with .ravel() \n",
    "          applied to each gradient). \n",
    "          If there are no params, return []\n",
    "        \"\"\"\n",
    "        # The gradient of an identity layer is precisely grad_output\n",
    "        input_dim = self.input_data.shape[1]\n",
    "        \n",
    "        d_layer_d_input = np.eye(input_dim)\n",
    "        \n",
    "        return np.dot(grad_output, d_layer_d_input), np.r_[[]] # chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Слой нелинейности ReLU\n",
    "\n",
    "Для начала реализуем слой нелинейности $ReLU(x) = max(x, 0)$. Параметров у слоя нет. Метод forward должен вернуть результат поэлементного применения ReLU к входному массиву, метод backward - градиент функции потерь по входу слоя. В нуле будем считать производную равной 0. Обратите внимание, что при обратном проходе могут понадобиться величины, посчитанные во время прямого прохода, поэтому их стоит сохранить как атрибут класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
    "        self.params = [] # ReLU has no parameters\n",
    "        self.negative_mask = None\n",
    "    \n",
    "    def __call__(self, input_data):\n",
    "        return self.forward(input_data)\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Apply elementwise ReLU to [batch, num_units] matrix\"\"\"\n",
    "        self.negative_mask = input_data <= 0.\n",
    "        input_clone = deepcopy(input_data)\n",
    "        input_clone[self.negative_mask] = 0.\n",
    "        return input_clone\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"Compute gradient of loss w.r.t. ReLU input\n",
    "        grad_output shape: [batch, num_units]\n",
    "        output 1 shape: [batch, num_units]\n",
    "        output 2: []\n",
    "        \"\"\"\n",
    "        return (1. - self.negative_mask) * grad_output, np.r_[[]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Полносвязный слой\n",
    "Далее реализуем полносвязный слой без нелинейности. У слоя два параметра: матрица весов и вектор сдвига.\n",
    "\n",
    "Обратите внимание на второй аргумент: в нем надо возвращать градиент по всем параметрам в одномерном виде. Для этого надо сначала применить .ravel() ко всем градиентам, а затем воспользоваться  np.r_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 2., 3.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "np.r_[np.eye(3).ravel(), np.arange(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    \"\"\"\n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self, input_units, output_units):\n",
    "        \"\"\"\n",
    "        A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = W x + b\n",
    "        \"\"\"\n",
    "        # initialize weights with small random numbers from normal distribution\n",
    "        self.weights = np.random.randn(input_units, output_units) * 0.01\n",
    "        self.biases = np.zeros(output_units)\n",
    "        self.params = [self.weights, self.biases]\n",
    "        self.input_data = None\n",
    "    \n",
    "    def __call__(self, input_data):\n",
    "        return self.forward(input_data)\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation:\n",
    "        f(x) = W x + b\n",
    "        \n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, output units]\n",
    "        \"\"\"\n",
    "        self.input_data = input_data\n",
    "        return np.dot(input_data, self.weights) + self.biases\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        compute gradients\n",
    "        grad_output shape: [batch, output_units]\n",
    "        output shapes: [batch, input_units], [num_params]\n",
    "        \n",
    "        hint: use function np.r_\n",
    "        np.r_[np.arange(3), np.arange(3)] = [0, 1, 2, 0, 1, 2]\n",
    "        \"\"\"\n",
    "        return (\n",
    "            np.dot(grad_output, self.weights.T), \n",
    "            np.r_[np.dot(self.input_data.T, grad_output).ravel(), np.sum(grad_output, axis=0).ravel()]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Проверка градиента\n",
    "\n",
    "Проверим правильность реализации с помощью функции численной проверки градиента. Функция берет на вход callable объект (функцию от одного аргумента-матрицы) и аргумент и вычисляет приближенный градиент функции в этой точке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x, verbose=False, h=0.00001):\n",
    "    \"\"\"Evaluates gradient df/dx via finite differences:\n",
    "    df/dx ~ (f(x+h) - f(x-h)) / 2h\n",
    "    Adopted from https://github.com/ddtm/dl-course/\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        \n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h # increment by h\n",
    "        fxph = f(x) # evaluate f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x) # evaluate f(x - h)\n",
    "        x[ix] = oldval # restore\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
    "        if verbose:\n",
    "            print (ix, grad[ix])\n",
    "        it.iternext() # step to next dimension\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Вычислите аналитический и численный градиенты по входу слоя ReLU от функции\n",
    "$$ f(y) = \\sum_i y_i, \\quad y = ReLU(x) $$\n",
    "\n",
    "Следующая ячейка после заполнения должна не выдавать ошибку :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "points = np.linspace(-1, 1, 10 * 12).reshape([10, 12])\n",
    "r = ReLU()\n",
    "\n",
    "r(points)\n",
    "grads = r.backward(np.ones_like(points))[0]\n",
    "numeric_grads = eval_numerical_gradient(lambda x: np.sum(r(x)), points)\n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Вычислите аналитический и численный градиенты по входу полносвязного слоя от функции\n",
    "$$ f(y) = \\sum_i y_i, \\quad y = W x + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 10 * 12).reshape([10, 12])\n",
    "l = Dense(12, 32)\n",
    "\n",
    "l(x)\n",
    "grads = l.backward(np.ones([10, 32]))[0]\n",
    "numeric_grads = eval_numerical_gradient(lambda x: np.sum(l(x)), points)\n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Реализация softmax-слоя и функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Для решения задачи многоклассовой классификации обычно используют softmax в качестве нелинейности на последнем слое, чтобы получить вероятности классов для каждого объекта:\n",
    "$$\\hat y = softmax(x)  = \\bigl \\{\\frac {exp(x_i)}{\\sum_j exp(x_j)} \\bigr \\}_{i=1}^K, \\quad K - \\text{число классов}$$\n",
    "В этом случае удобно оптимизировать логарифм правдоподобия:\n",
    "$$L(y, \\hat y) = -\\sum_{i=1}^K y_i \\log \\hat y_i \\rightarrow \\min,$$\n",
    "где $y_i=1$, если объект принадлежит $i$-му классу, и 0 иначе. Записанная в таком виде, эта функция потерь совпадает с выражением для кросс-энтропии. Очевидно, что ее также можно переписать через индексацию, если через $y_i$ обозначить класс данного объекта:\n",
    "$$L(y, \\hat y) = - \\log \\hat y_{y_i} \\rightarrow \\min$$\n",
    "В таком виде ее удобно реализовывать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Реализуйте слой Softmax (без параметров). Метод forward должен вычислять логарифм от softmax, а метод backward - пропускать градиенты. В общем случае в промежуточных вычислениях backward получится трехмерный тензор, однако для нашей конкретной функции потерь все вычисления можно реализовать в матричном виде.  Поэтому мы будем предполагать, что аргумент grad_output - это матрица, у которой в каждой строке только одно ненулевое значение (не обязательно единица)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "from scipy.special import softmax\n",
    "# use this function instead of np.log(np.sum(np.exp(...))) !\n",
    "# because it is more stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.input_data = None\n",
    "        \n",
    "    def __call__(self, input_data):\n",
    "        return self.forward(input_data)\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        Applies softmax to each row and then applies component-wise log\n",
    "        Input shape: [batch, num_units]\n",
    "        Output shape: [batch, num_units]\n",
    "        \"\"\"\n",
    "        max_value = np.max(input_data, axis=1).reshape(-1, 1)\n",
    "        self.input_data = input_data - max_value\n",
    "        return self.input_data - logsumexp(self.input_data, axis=1).reshape(-1, 1)\n",
    "        \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Propagartes gradients.\n",
    "        Assumes that each row of grad_output contains only 1\n",
    "        non-zero element\n",
    "        Input shape: [batch, num_units]\n",
    "        Output shape: [batch, num_units]\n",
    "        Do not forget to return [] as second value (grad w.r.t. params)\n",
    "        \"\"\"\n",
    "        return grad_output - np.sum(grad_output, axis=1).reshape(-1, 1) * softmax(self.input_data, axis=1), np.r_[[]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Реализуйте функцию потерь и градиенты функции потерь. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def crossentropy(activations, target):\n",
    "    \"\"\"\n",
    "    returns negative log-likelihood of target under model represented by\n",
    "    activations (log probabilities of classes)\n",
    "    each arg has shape [batch, num_classes]\n",
    "    output shape: 1 (scalar)\n",
    "    \"\"\"\n",
    "    return -np.sum(activations * target)\n",
    "    \n",
    "\n",
    "def grad_crossentropy(activations, target):\n",
    "    \"\"\"\n",
    "    returns gradient of negative log-likelihood w.r.t. activations\n",
    "    each arg has shape [batch, num_classes]\n",
    "    output shape: [batch, num_classes]\n",
    "    \n",
    "    hint: this is just one-hot encoding of target vector\n",
    "          multiplied by -1\n",
    "    \"\"\"\n",
    "    return -target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Наконец, выполните проверку softmax-слоя, используя функцию потерь и ее градиент.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "points = np.linspace(-1, 1, 10 * 12).reshape([10, 12])\n",
    "s = Softmax()\n",
    "\n",
    "target = np.arange(10)\n",
    "target = np.eye(12)[target]\n",
    "\n",
    "grads = s.backward(grad_crossentropy(s(points), target))[0]\n",
    "numeric_grads = eval_numerical_gradient(lambda x: crossentropy(s(x), target), points)\n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Загрузка данных\n",
    "\n",
    "Мы реализаовали все архитектурные составляющие нашей нейронной сети. Осталось загрузить данные и обучить модель. Мы будем работать с датасетом digits, каждый объект в котором - это 8x8 изображение рукописной цифры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "y = np.eye(10)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), (1797, 10))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Разделим данные на обучение и контроль:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1347, 64), (450, 64))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Сборка и обучение нейронной сети (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "В нашей реализации нейросеть - это список слоев. Например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_network(hidden_layers_size, n_layers):\n",
    "    network = []\n",
    "    if n_layers == 1:\n",
    "        network.append(Dense(X_train.shape[1], 10))\n",
    "    else:\n",
    "        network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "        for _ in range(n_layers - 2):\n",
    "            network.append(ReLU())\n",
    "            network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "        network.append(ReLU())\n",
    "        network.append(Dense(hidden_layers_size, 10))\n",
    "    network.append(Softmax())\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = make_network(32, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Для проверки, хорошо ли сеть обучилась, нам понадобится вычислять точность (accuracy) на данной выборке. Для этого реализуйте функцию, которая делает предсказания на каждом объекте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def predict_logits(network, X):\n",
    "    for layer in network:\n",
    "        X = layer(X)\n",
    "    return X\n",
    "\n",
    "def predict(network, X):\n",
    "    \"\"\"\n",
    "    returns predictions for each object in X\n",
    "    network: list of layers\n",
    "    X: raw data\n",
    "    X shape: [batch, features_num]\n",
    "    output: array of classes, each from 0 to 9\n",
    "    output shape: [batch]\n",
    "    \"\"\"\n",
    "    logits = predict_logits(network, X)\n",
    "    return np.argmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Мы будем обучать параметры нейросети с помощью готовой функции оптимизации из модуля scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function minimize in module scipy.optimize._minimize:\n",
      "\n",
      "minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)\n",
      "    Minimization of scalar function of one or more variables.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    fun : callable\n",
      "        The objective function to be minimized.\n",
      "    \n",
      "            ``fun(x, *args) -> float``\n",
      "    \n",
      "        where x is an 1-D array with shape (n,) and `args`\n",
      "        is a tuple of the fixed parameters needed to completely\n",
      "        specify the function.\n",
      "    x0 : ndarray, shape (n,)\n",
      "        Initial guess. Array of real elements of size (n,),\n",
      "        where 'n' is the number of independent variables.\n",
      "    args : tuple, optional\n",
      "        Extra arguments passed to the objective function and its\n",
      "        derivatives (`fun`, `jac` and `hess` functions).\n",
      "    method : str or callable, optional\n",
      "        Type of solver.  Should be one of\n",
      "    \n",
      "            - 'Nelder-Mead' :ref:`(see here) <optimize.minimize-neldermead>`\n",
      "            - 'Powell'      :ref:`(see here) <optimize.minimize-powell>`\n",
      "            - 'CG'          :ref:`(see here) <optimize.minimize-cg>`\n",
      "            - 'BFGS'        :ref:`(see here) <optimize.minimize-bfgs>`\n",
      "            - 'Newton-CG'   :ref:`(see here) <optimize.minimize-newtoncg>`\n",
      "            - 'L-BFGS-B'    :ref:`(see here) <optimize.minimize-lbfgsb>`\n",
      "            - 'TNC'         :ref:`(see here) <optimize.minimize-tnc>`\n",
      "            - 'COBYLA'      :ref:`(see here) <optimize.minimize-cobyla>`\n",
      "            - 'SLSQP'       :ref:`(see here) <optimize.minimize-slsqp>`\n",
      "            - 'trust-constr':ref:`(see here) <optimize.minimize-trustconstr>`\n",
      "            - 'dogleg'      :ref:`(see here) <optimize.minimize-dogleg>`\n",
      "            - 'trust-ncg'   :ref:`(see here) <optimize.minimize-trustncg>`\n",
      "            - 'trust-exact' :ref:`(see here) <optimize.minimize-trustexact>`\n",
      "            - 'trust-krylov' :ref:`(see here) <optimize.minimize-trustkrylov>`\n",
      "            - custom - a callable object (added in version 0.14.0),\n",
      "              see below for description.\n",
      "    \n",
      "        If not given, chosen to be one of ``BFGS``, ``L-BFGS-B``, ``SLSQP``,\n",
      "        depending if the problem has constraints or bounds.\n",
      "    jac : {callable,  '2-point', '3-point', 'cs', bool}, optional\n",
      "        Method for computing the gradient vector. Only for CG, BFGS,\n",
      "        Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg, trust-krylov,\n",
      "        trust-exact and trust-constr. If it is a callable, it should be a\n",
      "        function that returns the gradient vector:\n",
      "    \n",
      "            ``jac(x, *args) -> array_like, shape (n,)``\n",
      "    \n",
      "        where x is an array with shape (n,) and `args` is a tuple with\n",
      "        the fixed parameters. Alternatively, the keywords\n",
      "        {'2-point', '3-point', 'cs'} select a finite\n",
      "        difference scheme for numerical estimation of the gradient. Options\n",
      "        '3-point' and 'cs' are available only to 'trust-constr'.\n",
      "        If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
      "        gradient along with the objective function. If False, the gradient\n",
      "        will be estimated using '2-point' finite difference estimation.\n",
      "    hess : {callable, '2-point', '3-point', 'cs', HessianUpdateStrategy},  optional\n",
      "        Method for computing the Hessian matrix. Only for Newton-CG, dogleg,\n",
      "        trust-ncg,  trust-krylov, trust-exact and trust-constr. If it is\n",
      "        callable, it should return the  Hessian matrix:\n",
      "    \n",
      "            ``hess(x, *args) -> {LinearOperator, spmatrix, array}, (n, n)``\n",
      "    \n",
      "        where x is a (n,) ndarray and `args` is a tuple with the fixed\n",
      "        parameters. LinearOperator and sparse matrix returns are\n",
      "        allowed only for 'trust-constr' method. Alternatively, the keywords\n",
      "        {'2-point', '3-point', 'cs'} select a finite difference scheme\n",
      "        for numerical estimation. Or, objects implementing\n",
      "        `HessianUpdateStrategy` interface can be used to approximate\n",
      "        the Hessian. Available quasi-Newton methods implementing\n",
      "        this interface are:\n",
      "    \n",
      "            - `BFGS`;\n",
      "            - `SR1`.\n",
      "    \n",
      "        Whenever the gradient is estimated via finite-differences,\n",
      "        the Hessian cannot be estimated with options\n",
      "        {'2-point', '3-point', 'cs'} and needs to be\n",
      "        estimated using one of the quasi-Newton strategies.\n",
      "        Finite-difference options {'2-point', '3-point', 'cs'} and\n",
      "        `HessianUpdateStrategy` are available only for 'trust-constr' method.\n",
      "    hessp : callable, optional\n",
      "        Hessian of objective function times an arbitrary vector p. Only for\n",
      "        Newton-CG, trust-ncg, trust-krylov, trust-constr.\n",
      "        Only one of `hessp` or `hess` needs to be given.  If `hess` is\n",
      "        provided, then `hessp` will be ignored.  `hessp` must compute the\n",
      "        Hessian times an arbitrary vector:\n",
      "    \n",
      "            ``hessp(x, p, *args) ->  ndarray shape (n,)``\n",
      "    \n",
      "        where x is a (n,) ndarray, p is an arbitrary vector with\n",
      "        dimension (n,) and `args` is a tuple with the fixed\n",
      "        parameters.\n",
      "    bounds : sequence or `Bounds`, optional\n",
      "        Bounds on variables for L-BFGS-B, TNC, SLSQP and\n",
      "        trust-constr methods. There are two ways to specify the bounds:\n",
      "    \n",
      "            1. Instance of `Bounds` class.\n",
      "            2. Sequence of ``(min, max)`` pairs for each element in `x`. None\n",
      "               is used to specify no bound.\n",
      "    \n",
      "    constraints : {Constraint, dict} or List of {Constraint, dict}, optional\n",
      "        Constraints definition (only for COBYLA, SLSQP and trust-constr).\n",
      "        Constraints for 'trust-constr' are defined as a single object or a\n",
      "        list of objects specifying constraints to the optimization problem.\n",
      "        Available constraints are:\n",
      "    \n",
      "            - `LinearConstraint`\n",
      "            - `NonlinearConstraint`\n",
      "    \n",
      "        Constraints for COBYLA, SLSQP are defined as a list of dictionaries.\n",
      "        Each dictionary with fields:\n",
      "    \n",
      "            type : str\n",
      "                Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
      "            fun : callable\n",
      "                The function defining the constraint.\n",
      "            jac : callable, optional\n",
      "                The Jacobian of `fun` (only for SLSQP).\n",
      "            args : sequence, optional\n",
      "                Extra arguments to be passed to the function and Jacobian.\n",
      "    \n",
      "        Equality constraint means that the constraint function result is to\n",
      "        be zero whereas inequality means that it is to be non-negative.\n",
      "        Note that COBYLA only supports inequality constraints.\n",
      "    tol : float, optional\n",
      "        Tolerance for termination. For detailed control, use solver-specific\n",
      "        options.\n",
      "    options : dict, optional\n",
      "        A dictionary of solver options. All methods accept the following\n",
      "        generic options:\n",
      "    \n",
      "            maxiter : int\n",
      "                Maximum number of iterations to perform.\n",
      "            disp : bool\n",
      "                Set to True to print convergence messages.\n",
      "    \n",
      "        For method-specific options, see :func:`show_options()`.\n",
      "    callback : callable, optional\n",
      "        Called after each iteration. For 'trust-constr' it is a callable with\n",
      "        the signature:\n",
      "    \n",
      "            ``callback(xk, OptimizeResult state) -> bool``\n",
      "    \n",
      "        where ``xk`` is the current parameter vector. and ``state``\n",
      "        is an `OptimizeResult` object, with the same fields\n",
      "        as the ones from the return.  If callback returns True\n",
      "        the algorithm execution is terminated.\n",
      "        For all the other methods, the signature is:\n",
      "    \n",
      "            ``callback(xk)``\n",
      "    \n",
      "        where ``xk`` is the current parameter vector.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    res : OptimizeResult\n",
      "        The optimization result represented as a ``OptimizeResult`` object.\n",
      "        Important attributes are: ``x`` the solution array, ``success`` a\n",
      "        Boolean flag indicating if the optimizer exited successfully and\n",
      "        ``message`` which describes the cause of the termination. See\n",
      "        `OptimizeResult` for a description of other attributes.\n",
      "    \n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    minimize_scalar : Interface to minimization algorithms for scalar\n",
      "        univariate functions\n",
      "    show_options : Additional options accepted by the solvers\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This section describes the available solvers that can be selected by the\n",
      "    'method' parameter. The default method is *BFGS*.\n",
      "    \n",
      "    **Unconstrained minimization**\n",
      "    \n",
      "    Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the\n",
      "    Simplex algorithm [1]_, [2]_. This algorithm is robust in many\n",
      "    applications. However, if numerical computation of derivative can be\n",
      "    trusted, other algorithms using the first and/or second derivatives\n",
      "    information might be preferred for their better performance in\n",
      "    general.\n",
      "    \n",
      "    Method :ref:`Powell <optimize.minimize-powell>` is a modification\n",
      "    of Powell's method [3]_, [4]_ which is a conjugate direction\n",
      "    method. It performs sequential one-dimensional minimizations along\n",
      "    each vector of the directions set (`direc` field in `options` and\n",
      "    `info`), which is updated at each iteration of the main\n",
      "    minimization loop. The function need not be differentiable, and no\n",
      "    derivatives are taken.\n",
      "    \n",
      "    Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n",
      "    gradient algorithm by Polak and Ribiere, a variant of the\n",
      "    Fletcher-Reeves method described in [5]_ pp.  120-122. Only the\n",
      "    first derivatives are used.\n",
      "    \n",
      "    Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton\n",
      "    method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_\n",
      "    pp. 136. It uses the first derivatives only. BFGS has proven good\n",
      "    performance even for non-smooth optimizations. This method also\n",
      "    returns an approximation of the Hessian inverse, stored as\n",
      "    `hess_inv` in the OptimizeResult object.\n",
      "    \n",
      "    Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a\n",
      "    Newton-CG algorithm [5]_ pp. 168 (also known as the truncated\n",
      "    Newton method). It uses a CG method to the compute the search\n",
      "    direction. See also *TNC* method for a box-constrained\n",
      "    minimization with a similar algorithm. Suitable for large-scale\n",
      "    problems.\n",
      "    \n",
      "    Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg\n",
      "    trust-region algorithm [5]_ for unconstrained minimization. This\n",
      "    algorithm requires the gradient and Hessian; furthermore the\n",
      "    Hessian is required to be positive definite.\n",
      "    \n",
      "    Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the\n",
      "    Newton conjugate gradient trust-region algorithm [5]_ for\n",
      "    unconstrained minimization. This algorithm requires the gradient\n",
      "    and either the Hessian or a function that computes the product of\n",
      "    the Hessian with a given vector. Suitable for large-scale problems.\n",
      "    \n",
      "    Method :ref:`trust-krylov <optimize.minimize-trustkrylov>` uses\n",
      "    the Newton GLTR trust-region algorithm [14]_, [15]_ for unconstrained\n",
      "    minimization. This algorithm requires the gradient\n",
      "    and either the Hessian or a function that computes the product of\n",
      "    the Hessian with a given vector. Suitable for large-scale problems.\n",
      "    On indefinite problems it requires usually less iterations than the\n",
      "    `trust-ncg` method and is recommended for medium and large-scale problems.\n",
      "    \n",
      "    Method :ref:`trust-exact <optimize.minimize-trustexact>`\n",
      "    is a trust-region method for unconstrained minimization in which\n",
      "    quadratic subproblems are solved almost exactly [13]_. This\n",
      "    algorithm requires the gradient and the Hessian (which is\n",
      "    *not* required to be positive definite). It is, in many\n",
      "    situations, the Newton method to converge in fewer iteraction\n",
      "    and the most recommended for small and medium-size problems.\n",
      "    \n",
      "    **Bound-Constrained minimization**\n",
      "    \n",
      "    Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B\n",
      "    algorithm [6]_, [7]_ for bound constrained minimization.\n",
      "    \n",
      "    Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton\n",
      "    algorithm [5]_, [8]_ to minimize a function with variables subject\n",
      "    to bounds. This algorithm uses gradient information; it is also\n",
      "    called Newton Conjugate-Gradient. It differs from the *Newton-CG*\n",
      "    method described above as it wraps a C implementation and allows\n",
      "    each variable to be given upper and lower bounds.\n",
      "    \n",
      "    **Constrained Minimization**\n",
      "    \n",
      "    Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the\n",
      "    Constrained Optimization BY Linear Approximation (COBYLA) method\n",
      "    [9]_, [10]_, [11]_. The algorithm is based on linear\n",
      "    approximations to the objective function and each constraint. The\n",
      "    method wraps a FORTRAN implementation of the algorithm. The\n",
      "    constraints functions 'fun' may return either a single number\n",
      "    or an array or list of numbers.\n",
      "    \n",
      "    Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential\n",
      "    Least SQuares Programming to minimize a function of several\n",
      "    variables with any combination of bounds, equality and inequality\n",
      "    constraints. The method wraps the SLSQP Optimization subroutine\n",
      "    originally implemented by Dieter Kraft [12]_. Note that the\n",
      "    wrapper handles infinite values in bounds by converting them into\n",
      "    large floating values.\n",
      "    \n",
      "    Method :ref:`trust-constr <optimize.minimize-trustconstr>` is a\n",
      "    trust-region algorithm for constrained optimization. It swiches\n",
      "    between two implementations depending on the problem definition.\n",
      "    It is the most versatile constrained minimization algorithm\n",
      "    implemented in SciPy and the most appropriate for large-scale problems.\n",
      "    For equality constrained problems it is an implementation of Byrd-Omojokun\n",
      "    Trust-Region SQP method described in [17]_ and in [5]_, p. 549. When\n",
      "    inequality constraints  are imposed as well, it swiches to the trust-region\n",
      "    interior point  method described in [16]_. This interior point algorithm,\n",
      "    in turn, solves inequality constraints by introducing slack variables\n",
      "    and solving a sequence of equality-constrained barrier problems\n",
      "    for progressively smaller values of the barrier parameter.\n",
      "    The previously described equality constrained SQP method is\n",
      "    used to solve the subproblems with increasing levels of accuracy\n",
      "    as the iterate gets closer to a solution.\n",
      "    \n",
      "    **Finite-Difference Options**\n",
      "    \n",
      "    For Method :ref:`trust-constr <optimize.minimize-trustconstr>`\n",
      "    the gradient and the Hessian may be approximated using\n",
      "    three finite-difference schemes: {'2-point', '3-point', 'cs'}.\n",
      "    The scheme 'cs' is, potentially, the most accurate but it\n",
      "    requires the function to correctly handles complex inputs and to\n",
      "    be differentiable in the complex plane. The scheme '3-point' is more\n",
      "    accurate than '2-point' but requires twice as much operations.\n",
      "    \n",
      "    **Custom minimizers**\n",
      "    \n",
      "    It may be useful to pass a custom minimization method, for example\n",
      "    when using a frontend to this method such as `scipy.optimize.basinhopping`\n",
      "    or a different library.  You can simply pass a callable as the ``method``\n",
      "    parameter.\n",
      "    \n",
      "    The callable is called as ``method(fun, x0, args, **kwargs, **options)``\n",
      "    where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "    (such as `callback`, `hess`, etc.), except the `options` dict, which has\n",
      "    its contents also passed as `method` parameters pair by pair.  Also, if\n",
      "    `jac` has been passed as a bool type, `jac` and `fun` are mangled so that\n",
      "    `fun` returns just the function values and `jac` is converted to a function\n",
      "    returning the Jacobian.  The method shall return an ``OptimizeResult``\n",
      "    object.\n",
      "    \n",
      "    The provided `method` callable must be able to accept (and possibly ignore)\n",
      "    arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "    expand in future versions and then these parameters will be passed to\n",
      "    the method.  You can find an example in the scipy.optimize tutorial.\n",
      "    \n",
      "    .. versionadded:: 0.11.0\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
      "        Minimization. The Computer Journal 7: 308-13.\n",
      "    .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
      "        respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
      "        Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
      "        Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
      "        191-208.\n",
      "    .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n",
      "       a function of several variables without calculating derivatives. The\n",
      "       Computer Journal 7: 155-162.\n",
      "    .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n",
      "       Numerical Recipes (any edition), Cambridge University Press.\n",
      "    .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
      "       Springer New York.\n",
      "    .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n",
      "       Algorithm for Bound Constrained Optimization. SIAM Journal on\n",
      "       Scientific and Statistical Computing 16 (5): 1190-1208.\n",
      "    .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n",
      "       778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n",
      "       optimization. ACM Transactions on Mathematical Software 23 (4):\n",
      "       550-560.\n",
      "    .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n",
      "       1984. SIAM Journal of Numerical Analysis 21: 770-778.\n",
      "    .. [9] Powell, M J D. A direct search optimization method that models\n",
      "       the objective and constraint functions by linear interpolation.\n",
      "       1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n",
      "       and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n",
      "    .. [10] Powell M J D. Direct search algorithms for optimization\n",
      "       calculations. 1998. Acta Numerica 7: 287-336.\n",
      "    .. [11] Powell M J D. A view of algorithms for optimization without\n",
      "       derivatives. 2007.Cambridge University Technical Report DAMTP\n",
      "       2007/NA03\n",
      "    .. [12] Kraft, D. A software package for sequential quadratic\n",
      "       programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n",
      "       Center -- Institute for Flight Mechanics, Koln, Germany.\n",
      "    .. [13] Conn, A. R., Gould, N. I., and Toint, P. L.\n",
      "       Trust region methods. 2000. Siam. pp. 169-200.\n",
      "    .. [14] F. Lenders, C. Kirches, A. Potschka: \"trlib: A vector-free\n",
      "       implementation of the GLTR method for iterative solution of\n",
      "       the trust region problem\", https://arxiv.org/abs/1611.04718\n",
      "    .. [15] N. Gould, S. Lucidi, M. Roma, P. Toint: \"Solving the\n",
      "       Trust-Region Subproblem using the Lanczos Method\",\n",
      "       SIAM J. Optim., 9(2), 504--525, (1999).\n",
      "    .. [16] Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999.\n",
      "        An interior point algorithm for large-scale nonlinear  programming.\n",
      "        SIAM Journal on Optimization 9.4: 877-900.\n",
      "    .. [17] Lalee, Marucha, Jorge Nocedal, and Todd Plantega. 1998. On the\n",
      "        implementation of an algorithm for large-scale equality constrained\n",
      "        optimization. SIAM Journal on Optimization 8.3: 682-706.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "    function (and its respective derivatives) is implemented in `rosen`\n",
      "    (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n",
      "    \n",
      "    >>> from scipy.optimize import minimize, rosen, rosen_der\n",
      "    \n",
      "    A simple application of the *Nelder-Mead* method is:\n",
      "    \n",
      "    >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
      "    >>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "    \n",
      "    Now using the *BFGS* algorithm, using the first derivative and a few\n",
      "    options:\n",
      "    \n",
      "    >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
      "    ...                options={'gtol': 1e-6, 'disp': True})\n",
      "    Optimization terminated successfully.\n",
      "             Current function value: 0.000000\n",
      "             Iterations: 26\n",
      "             Function evaluations: 31\n",
      "             Gradient evaluations: 31\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "    >>> print(res.message)\n",
      "    Optimization terminated successfully.\n",
      "    >>> res.hess_inv\n",
      "    array([[ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary\n",
      "           [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],\n",
      "           [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],\n",
      "           [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],\n",
      "           [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]])\n",
      "    \n",
      "    \n",
      "    Next, consider a minimization problem with several constraints (namely\n",
      "    Example 16.4 from [5]_). The objective function is:\n",
      "    \n",
      "    >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
      "    \n",
      "    There are three constraints defined as:\n",
      "    \n",
      "    >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
      "    \n",
      "    And variables must be positive, hence the following bounds:\n",
      "    \n",
      "    >>> bnds = ((0, None), (0, None))\n",
      "    \n",
      "    The optimization problem is solved using the SLSQP method as:\n",
      "    \n",
      "    >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n",
      "    ...                constraints=cons)\n",
      "    \n",
      "    It should converge to the theoretical solution (1.4 ,1.7).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(minimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Эта функция имеет стандартный интерфейс: нужно передать callable объект, который вычисляет значение и градиент целевой функции, а также точку старта оптимизации - начальное приближение (одномерный numpy-массив). Поэтому нам понадобятся функции для сбора и задания всех весов нашей нейросети (именно для них мы всегда записывали параметры слоя в список layer.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def get_weights(network):\n",
    "    weights = []\n",
    "    for layer in network:\n",
    "        for param in layer.params:\n",
    "            weights += param.ravel().tolist()\n",
    "    return np.array(weights)\n",
    "\n",
    "def set_weights(weights, network):\n",
    "    i = 0\n",
    "    for layer in network:\n",
    "        for param in layer.params:\n",
    "            l = param.size\n",
    "            param[:] = weights[i:i+l].\\\n",
    "                             reshape(param.shape)\n",
    "            i += l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Вам нужно реализовать ту самую функцию, которую мы будем передавать в minimize. Эта функция должна брать на вход текущую точку (вектор всех параметров), а также список дополнительных параметров (мы будем передавать через них нашу сеть и обучающие данные) и возвращать значение критерия качества (кросс-энтропия) и его градиент по параметрам модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def compute_loss_grad(weights, args):\n",
    "    \"\"\"\n",
    "    takes current weights and computes cross-entropy and gradients\n",
    "    weights shape: [num_parameters]\n",
    "    output 1: loss (scalar)\n",
    "    output 2: gradint w.r.t. weights, shape: [num_parameters]\n",
    "    \n",
    "    hint: firstly perform forward pass through the whole network\n",
    "    then compute loss and its gradients\n",
    "    then perform backward pass, transmitting first baskward output\n",
    "    to the previos layer and saving second baskward output in a list\n",
    "    finally flatten all the gradients in this list\n",
    "    (in the order from the first to the last layer)\n",
    "    \n",
    "    Do not forget to set weights of the network!\n",
    "    \"\"\"\n",
    "    network, X, y = args\n",
    "    set_weights(weights, network)\n",
    "    \n",
    "    log_probas = predict_logits(network, X)\n",
    "    cross_entropy = crossentropy(log_probas, y)\n",
    "    grad_output = grad_crossentropy(log_probas, y)\n",
    "    \n",
    "    weights = []\n",
    "    for layer in reversed(network):\n",
    "        grad_output, current_weights = layer.backward(grad_output)\n",
    "        weights.append(current_weights)\n",
    "\n",
    "    return cross_entropy, np.concatenate(weights[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Теперь мы готовы обучать нашу нейросеть. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "weights = get_weights(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "res = minimize(\n",
    "    compute_loss_grad, weights,  # fun and start point\n",
    "    args=[network, X_train, y_train], # args passed to fun\n",
    "    method=\"L-BFGS-B\", # optimization method\n",
    "    jac=True, # says that gradient are computed in fun\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fun', 'jac', 'nfev', 'nit', 'status', 'message', 'x', 'success', 'hess_inv'])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"nit\"] # number of iterations (should be >> 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"success\"] # should be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.37125954e-04,  2.49100669e-03,  1.64583432e-03, ...,\n",
       "        3.73294963e-01, -7.95842198e-01,  2.70653473e-01])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"x\"] # leraned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Выведите качество на обучении (X_train, y_train) и на контроле (X_test, y_test. Не забудьте установить веса!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy (Train/Test): 0.000/1073.153\n",
      "Accuracy (Train/Test): 1.000/0.924\n"
     ]
    }
   ],
   "source": [
    "set_weights(res['x'], network)\n",
    "print(\n",
    "    \"Cross Entropy (Train/Test): {0:.3f}/{1:.3f}\".format(\n",
    "        crossentropy(predict_logits(network, X_train), y_train),\n",
    "        crossentropy(predict_logits(network, X_test), y_test)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Accuracy (Train/Test): {0:.3f}/{1:.3f}\".format(\n",
    "        np.mean(predict(network, X_train) == np.argmax(y_train, axis=1)),\n",
    "        np.mean(predict(network, X_test) == np.argmax(y_test, axis=1))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "У minimize есть также аргумент callback - в нее можно передать функцию, которая будет вызываться после каждой итерации оптимизации. Такую функцию удобно оформить в виде метода класса, который будет сохранять качество на обучении контроле после каждой итерации. Реализуйте этот метод в классе Callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    def __init__(self, network, X_train, y_train, X_test, y_test, print=False):\n",
    "        self.iteration = 0\n",
    "        self.network = network\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.print = print\n",
    "        self.train_acc = []\n",
    "        self.test_acc = []\n",
    "        self.train_ce = []\n",
    "        self.test_ce = []\n",
    "        \n",
    "    def call(self, weights):\n",
    "        \"\"\"\n",
    "        computes quality on train and test set with given weights\n",
    "        and saves to self.train_acc and self.test_acc\n",
    "        if self.print is True, also prints these 2 values\n",
    "        \"\"\"        \n",
    "        self.iteration += 1\n",
    "        set_weights(weights, self.network)\n",
    "        \n",
    "        self.train_acc.append(np.mean(predict(network, X_train) == np.argmax(y_train, axis=1)))\n",
    "        self.test_acc.append(np.mean(predict(network, X_test) == np.argmax(y_test, axis=1)))\n",
    "        \n",
    "        self.train_ce.append(crossentropy(predict_logits(network, X_train), y_train))\n",
    "        self.test_ce.append(crossentropy(predict_logits(network, X_test), y_test))\n",
    "        if self.print:\n",
    "            print('\\n Iteration: {0:d}'.format(self.iteration))\n",
    "            print('Cross Entropy (Train/Test): {0:.3f}/{1:.3f}'.format(self.train_ce[-1], self.test_ce[-1]))\n",
    "            print('Accuracy (Train/Test): {0:.3f}/{1:.3f}'.format(self.train_acc[-1], self.test_acc[-1]))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Iteration: 1\n",
      "Cross Entropy (Train/Test): 3092.361/1038.291\n",
      "Accuracy (Train/Test): 0.106/0.084\n",
      "\n",
      " Iteration: 2\n",
      "Cross Entropy (Train/Test): 3082.559/1040.939\n",
      "Accuracy (Train/Test): 0.181/0.151\n",
      "\n",
      " Iteration: 3\n",
      "Cross Entropy (Train/Test): 3078.683/1044.117\n",
      "Accuracy (Train/Test): 0.180/0.147\n",
      "\n",
      " Iteration: 4\n",
      "Cross Entropy (Train/Test): 3063.937/1043.869\n",
      "Accuracy (Train/Test): 0.132/0.102\n",
      "\n",
      " Iteration: 5\n",
      "Cross Entropy (Train/Test): 3039.576/1045.986\n",
      "Accuracy (Train/Test): 0.110/0.084\n",
      "\n",
      " Iteration: 6\n",
      "Cross Entropy (Train/Test): 2990.123/1031.504\n",
      "Accuracy (Train/Test): 0.112/0.087\n",
      "\n",
      " Iteration: 7\n",
      "Cross Entropy (Train/Test): 2799.418/964.645\n",
      "Accuracy (Train/Test): 0.223/0.196\n",
      "\n",
      " Iteration: 8\n",
      "Cross Entropy (Train/Test): 2476.952/853.545\n",
      "Accuracy (Train/Test): 0.258/0.247\n",
      "\n",
      " Iteration: 9\n",
      "Cross Entropy (Train/Test): 2267.691/780.969\n",
      "Accuracy (Train/Test): 0.355/0.338\n",
      "\n",
      " Iteration: 10\n",
      "Cross Entropy (Train/Test): 2115.892/724.607\n",
      "Accuracy (Train/Test): 0.471/0.458\n",
      "\n",
      " Iteration: 11\n",
      "Cross Entropy (Train/Test): 1820.139/624.137\n",
      "Accuracy (Train/Test): 0.555/0.516\n",
      "\n",
      " Iteration: 12\n",
      "Cross Entropy (Train/Test): 1648.797/560.334\n",
      "Accuracy (Train/Test): 0.600/0.569\n",
      "\n",
      " Iteration: 13\n",
      "Cross Entropy (Train/Test): 1438.189/486.168\n",
      "Accuracy (Train/Test): 0.681/0.669\n",
      "\n",
      " Iteration: 14\n",
      "Cross Entropy (Train/Test): 1267.824/432.199\n",
      "Accuracy (Train/Test): 0.696/0.682\n",
      "\n",
      " Iteration: 15\n",
      "Cross Entropy (Train/Test): 1167.145/397.608\n",
      "Accuracy (Train/Test): 0.719/0.680\n",
      "\n",
      " Iteration: 16\n",
      "Cross Entropy (Train/Test): 1090.328/368.162\n",
      "Accuracy (Train/Test): 0.766/0.733\n",
      "\n",
      " Iteration: 17\n",
      "Cross Entropy (Train/Test): 994.659/330.688\n",
      "Accuracy (Train/Test): 0.789/0.780\n",
      "\n",
      " Iteration: 18\n",
      "Cross Entropy (Train/Test): 913.068/306.224\n",
      "Accuracy (Train/Test): 0.788/0.769\n",
      "\n",
      " Iteration: 19\n",
      "Cross Entropy (Train/Test): 847.260/288.913\n",
      "Accuracy (Train/Test): 0.811/0.800\n",
      "\n",
      " Iteration: 20\n",
      "Cross Entropy (Train/Test): 807.948/280.868\n",
      "Accuracy (Train/Test): 0.815/0.818\n",
      "\n",
      " Iteration: 21\n",
      "Cross Entropy (Train/Test): 766.153/269.805\n",
      "Accuracy (Train/Test): 0.825/0.809\n",
      "\n",
      " Iteration: 22\n",
      "Cross Entropy (Train/Test): 714.546/256.378\n",
      "Accuracy (Train/Test): 0.834/0.833\n",
      "\n",
      " Iteration: 23\n",
      "Cross Entropy (Train/Test): 656.433/227.789\n",
      "Accuracy (Train/Test): 0.844/0.844\n",
      "\n",
      " Iteration: 24\n",
      "Cross Entropy (Train/Test): 578.922/207.956\n",
      "Accuracy (Train/Test): 0.862/0.858\n",
      "\n",
      " Iteration: 25\n",
      "Cross Entropy (Train/Test): 528.008/196.429\n",
      "Accuracy (Train/Test): 0.872/0.864\n",
      "\n",
      " Iteration: 26\n",
      "Cross Entropy (Train/Test): 480.670/181.219\n",
      "Accuracy (Train/Test): 0.886/0.860\n",
      "\n",
      " Iteration: 27\n",
      "Cross Entropy (Train/Test): 445.670/172.876\n",
      "Accuracy (Train/Test): 0.895/0.869\n",
      "\n",
      " Iteration: 28\n",
      "Cross Entropy (Train/Test): 423.779/163.539\n",
      "Accuracy (Train/Test): 0.898/0.882\n",
      "\n",
      " Iteration: 29\n",
      "Cross Entropy (Train/Test): 407.469/158.114\n",
      "Accuracy (Train/Test): 0.897/0.887\n",
      "\n",
      " Iteration: 30\n",
      "Cross Entropy (Train/Test): 385.086/150.530\n",
      "Accuracy (Train/Test): 0.904/0.898\n",
      "\n",
      " Iteration: 31\n",
      "Cross Entropy (Train/Test): 344.739/135.819\n",
      "Accuracy (Train/Test): 0.915/0.911\n",
      "\n",
      " Iteration: 32\n",
      "Cross Entropy (Train/Test): 325.327/140.384\n",
      "Accuracy (Train/Test): 0.918/0.904\n",
      "\n",
      " Iteration: 33\n",
      "Cross Entropy (Train/Test): 268.978/116.312\n",
      "Accuracy (Train/Test): 0.938/0.927\n",
      "\n",
      " Iteration: 34\n",
      "Cross Entropy (Train/Test): 252.349/110.865\n",
      "Accuracy (Train/Test): 0.942/0.936\n",
      "\n",
      " Iteration: 35\n",
      "Cross Entropy (Train/Test): 233.225/106.638\n",
      "Accuracy (Train/Test): 0.952/0.944\n",
      "\n",
      " Iteration: 36\n",
      "Cross Entropy (Train/Test): 214.629/102.853\n",
      "Accuracy (Train/Test): 0.954/0.944\n",
      "\n",
      " Iteration: 37\n",
      "Cross Entropy (Train/Test): 194.867/101.921\n",
      "Accuracy (Train/Test): 0.955/0.947\n",
      "\n",
      " Iteration: 38\n",
      "Cross Entropy (Train/Test): 178.553/100.740\n",
      "Accuracy (Train/Test): 0.959/0.936\n",
      "\n",
      " Iteration: 39\n",
      "Cross Entropy (Train/Test): 164.815/97.914\n",
      "Accuracy (Train/Test): 0.964/0.940\n",
      "\n",
      " Iteration: 40\n",
      "Cross Entropy (Train/Test): 149.104/97.012\n",
      "Accuracy (Train/Test): 0.965/0.942\n",
      "\n",
      " Iteration: 41\n",
      "Cross Entropy (Train/Test): 137.404/97.434\n",
      "Accuracy (Train/Test): 0.964/0.944\n",
      "\n",
      " Iteration: 42\n",
      "Cross Entropy (Train/Test): 122.418/94.236\n",
      "Accuracy (Train/Test): 0.968/0.942\n",
      "\n",
      " Iteration: 43\n",
      "Cross Entropy (Train/Test): 110.134/90.713\n",
      "Accuracy (Train/Test): 0.971/0.944\n",
      "\n",
      " Iteration: 44\n",
      "Cross Entropy (Train/Test): 105.246/91.845\n",
      "Accuracy (Train/Test): 0.976/0.951\n",
      "\n",
      " Iteration: 45\n",
      "Cross Entropy (Train/Test): 98.217/81.639\n",
      "Accuracy (Train/Test): 0.976/0.956\n",
      "\n",
      " Iteration: 46\n",
      "Cross Entropy (Train/Test): 96.145/81.364\n",
      "Accuracy (Train/Test): 0.976/0.956\n",
      "\n",
      " Iteration: 47\n",
      "Cross Entropy (Train/Test): 89.428/81.311\n",
      "Accuracy (Train/Test): 0.981/0.956\n",
      "\n",
      " Iteration: 48\n",
      "Cross Entropy (Train/Test): 81.604/81.882\n",
      "Accuracy (Train/Test): 0.983/0.953\n",
      "\n",
      " Iteration: 49\n",
      "Cross Entropy (Train/Test): 76.156/84.708\n",
      "Accuracy (Train/Test): 0.982/0.953\n",
      "\n",
      " Iteration: 50\n",
      "Cross Entropy (Train/Test): 67.290/84.837\n",
      "Accuracy (Train/Test): 0.986/0.956\n",
      "\n",
      " Iteration: 51\n",
      "Cross Entropy (Train/Test): 63.352/82.930\n",
      "Accuracy (Train/Test): 0.986/0.956\n",
      "\n",
      " Iteration: 52\n",
      "Cross Entropy (Train/Test): 56.502/81.247\n",
      "Accuracy (Train/Test): 0.987/0.953\n",
      "\n",
      " Iteration: 53\n",
      "Cross Entropy (Train/Test): 50.673/78.251\n",
      "Accuracy (Train/Test): 0.987/0.956\n",
      "\n",
      " Iteration: 54\n",
      "Cross Entropy (Train/Test): 43.724/76.029\n",
      "Accuracy (Train/Test): 0.990/0.956\n",
      "\n",
      " Iteration: 55\n",
      "Cross Entropy (Train/Test): 36.549/76.590\n",
      "Accuracy (Train/Test): 0.993/0.956\n",
      "\n",
      " Iteration: 56\n",
      "Cross Entropy (Train/Test): 34.828/70.034\n",
      "Accuracy (Train/Test): 0.993/0.956\n",
      "\n",
      " Iteration: 57\n",
      "Cross Entropy (Train/Test): 30.837/72.747\n",
      "Accuracy (Train/Test): 0.995/0.953\n",
      "\n",
      " Iteration: 58\n",
      "Cross Entropy (Train/Test): 29.162/71.561\n",
      "Accuracy (Train/Test): 0.996/0.958\n",
      "\n",
      " Iteration: 59\n",
      "Cross Entropy (Train/Test): 26.221/71.808\n",
      "Accuracy (Train/Test): 0.998/0.953\n",
      "\n",
      " Iteration: 60\n",
      "Cross Entropy (Train/Test): 23.524/72.087\n",
      "Accuracy (Train/Test): 0.998/0.956\n",
      "\n",
      " Iteration: 61\n",
      "Cross Entropy (Train/Test): 21.235/75.350\n",
      "Accuracy (Train/Test): 0.997/0.960\n",
      "\n",
      " Iteration: 62\n",
      "Cross Entropy (Train/Test): 17.033/75.347\n",
      "Accuracy (Train/Test): 0.999/0.960\n",
      "\n",
      " Iteration: 63\n",
      "Cross Entropy (Train/Test): 15.327/76.334\n",
      "Accuracy (Train/Test): 0.999/0.962\n",
      "\n",
      " Iteration: 64\n",
      "Cross Entropy (Train/Test): 13.111/79.400\n",
      "Accuracy (Train/Test): 0.999/0.960\n",
      "\n",
      " Iteration: 65\n",
      "Cross Entropy (Train/Test): 10.774/81.801\n",
      "Accuracy (Train/Test): 1.000/0.953\n",
      "\n",
      " Iteration: 66\n",
      "Cross Entropy (Train/Test): 9.170/88.558\n",
      "Accuracy (Train/Test): 1.000/0.951\n",
      "\n",
      " Iteration: 67\n",
      "Cross Entropy (Train/Test): 7.516/90.446\n",
      "Accuracy (Train/Test): 1.000/0.951\n",
      "\n",
      " Iteration: 68\n",
      "Cross Entropy (Train/Test): 6.670/92.603\n",
      "Accuracy (Train/Test): 1.000/0.949\n",
      "\n",
      " Iteration: 69\n",
      "Cross Entropy (Train/Test): 5.532/88.121\n",
      "Accuracy (Train/Test): 1.000/0.953\n",
      "\n",
      " Iteration: 70\n",
      "Cross Entropy (Train/Test): 5.153/88.435\n",
      "Accuracy (Train/Test): 1.000/0.953\n",
      "\n",
      " Iteration: 71\n",
      "Cross Entropy (Train/Test): 4.591/89.145\n",
      "Accuracy (Train/Test): 1.000/0.953\n",
      "\n",
      " Iteration: 72\n",
      "Cross Entropy (Train/Test): 3.956/89.915\n",
      "Accuracy (Train/Test): 1.000/0.956\n",
      "\n",
      " Iteration: 73\n",
      "Cross Entropy (Train/Test): 3.063/90.849\n",
      "Accuracy (Train/Test): 1.000/0.958\n",
      "\n",
      " Iteration: 74\n",
      "Cross Entropy (Train/Test): 2.356/91.670\n",
      "Accuracy (Train/Test): 1.000/0.956\n",
      "\n",
      " Iteration: 75\n",
      "Cross Entropy (Train/Test): 1.718/94.798\n",
      "Accuracy (Train/Test): 1.000/0.960\n",
      "\n",
      " Iteration: 76\n",
      "Cross Entropy (Train/Test): 1.428/94.947\n",
      "Accuracy (Train/Test): 1.000/0.960\n",
      "\n",
      " Iteration: 77\n",
      "Cross Entropy (Train/Test): 1.071/96.922\n",
      "Accuracy (Train/Test): 1.000/0.956\n",
      "\n",
      " Iteration: 78\n",
      "Cross Entropy (Train/Test): 0.808/99.864\n",
      "Accuracy (Train/Test): 1.000/0.958\n",
      "\n",
      " Iteration: 79\n",
      "Cross Entropy (Train/Test): 0.649/102.936\n",
      "Accuracy (Train/Test): 1.000/0.953\n",
      "\n",
      " Iteration: 80\n",
      "Cross Entropy (Train/Test): 0.543/105.960\n",
      "Accuracy (Train/Test): 1.000/0.956\n",
      "\n",
      " Iteration: 81\n",
      "Cross Entropy (Train/Test): 0.417/110.235\n",
      "Accuracy (Train/Test): 1.000/0.956\n",
      "\n",
      " Iteration: 82\n",
      "Cross Entropy (Train/Test): 0.357/116.548\n",
      "Accuracy (Train/Test): 1.000/0.960\n",
      "\n",
      " Iteration: 83\n",
      "Cross Entropy (Train/Test): 0.261/120.357\n",
      "Accuracy (Train/Test): 1.000/0.958\n",
      "\n",
      " Iteration: 84\n",
      "Cross Entropy (Train/Test): 0.197/124.608\n",
      "Accuracy (Train/Test): 1.000/0.960\n",
      "\n",
      " Iteration: 85\n",
      "Cross Entropy (Train/Test): 0.140/130.051\n",
      "Accuracy (Train/Test): 1.000/0.960\n",
      "\n",
      " Iteration: 86\n",
      "Cross Entropy (Train/Test): 0.096/133.432\n",
      "Accuracy (Train/Test): 1.000/0.960\n",
      "\n",
      " Iteration: 87\n",
      "Cross Entropy (Train/Test): 0.063/143.293\n",
      "Accuracy (Train/Test): 1.000/0.962\n",
      "\n",
      " Iteration: 88\n",
      "Cross Entropy (Train/Test): 0.047/149.238\n",
      "Accuracy (Train/Test): 1.000/0.962\n",
      "\n",
      " Iteration: 89\n",
      "Cross Entropy (Train/Test): 0.026/157.763\n",
      "Accuracy (Train/Test): 1.000/0.962\n",
      "\n",
      " Iteration: 90\n",
      "Cross Entropy (Train/Test): 0.015/165.214\n",
      "Accuracy (Train/Test): 1.000/0.964\n",
      "\n",
      " Iteration: 91\n",
      "Cross Entropy (Train/Test): 0.008/171.954\n",
      "Accuracy (Train/Test): 1.000/0.964\n",
      "\n",
      " Iteration: 92\n",
      "Cross Entropy (Train/Test): 0.005/179.239\n",
      "Accuracy (Train/Test): 1.000/0.964\n",
      "\n",
      " Iteration: 93\n",
      "Cross Entropy (Train/Test): 0.003/188.459\n",
      "Accuracy (Train/Test): 1.000/0.962\n",
      "\n",
      " Iteration: 94\n",
      "Cross Entropy (Train/Test): 0.002/205.749\n",
      "Accuracy (Train/Test): 1.000/0.964\n",
      "\n",
      " Iteration: 95\n",
      "Cross Entropy (Train/Test): 0.001/209.072\n",
      "Accuracy (Train/Test): 1.000/0.964\n",
      "\n",
      " Iteration: 96\n",
      "Cross Entropy (Train/Test): 0.001/213.101\n",
      "Accuracy (Train/Test): 1.000/0.964\n",
      "\n",
      " Iteration: 97\n",
      "Cross Entropy (Train/Test): 0.000/222.116\n",
      "Accuracy (Train/Test): 1.000/0.962\n",
      "\n",
      " Iteration: 98\n",
      "Cross Entropy (Train/Test): 0.000/230.391\n",
      "Accuracy (Train/Test): 1.000/0.962\n",
      "\n",
      " Iteration: 99\n",
      "Cross Entropy (Train/Test): 0.000/250.286\n",
      "Accuracy (Train/Test): 1.000/0.960\n",
      "\n",
      " Iteration: 100\n",
      "Cross Entropy (Train/Test): 0.000/259.272\n",
      "Accuracy (Train/Test): 1.000/0.960\n",
      "\n",
      " Iteration: 101\n",
      "Cross Entropy (Train/Test): 0.000/275.997\n",
      "Accuracy (Train/Test): 1.000/0.960\n",
      "\n",
      " Iteration: 102\n",
      "Cross Entropy (Train/Test): 0.000/289.450\n",
      "Accuracy (Train/Test): 1.000/0.962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Iteration: 103\n",
      "Cross Entropy (Train/Test): 0.000/303.992\n",
      "Accuracy (Train/Test): 1.000/0.960\n",
      "\n",
      " Iteration: 104\n",
      "Cross Entropy (Train/Test): 0.000/318.745\n",
      "Accuracy (Train/Test): 1.000/0.960\n",
      "\n",
      " Iteration: 105\n",
      "Cross Entropy (Train/Test): 0.000/335.411\n",
      "Accuracy (Train/Test): 1.000/0.962\n",
      "\n",
      " Iteration: 106\n",
      "Cross Entropy (Train/Test): 0.000/357.861\n",
      "Accuracy (Train/Test): 1.000/0.960\n",
      "\n",
      " Iteration: 107\n",
      "Cross Entropy (Train/Test): 0.000/372.708\n",
      "Accuracy (Train/Test): 1.000/0.960\n"
     ]
    }
   ],
   "source": [
    "network = make_network(32, 3)\n",
    "weights = get_weights(network)\n",
    "cb = Callback(network, X_train, y_train, X_test, y_test, print=True)\n",
    "res = minimize(\n",
    "    compute_loss_grad, weights,\n",
    "    args=[network, X_train, y_train],\n",
    "    method=\"L-BFGS-B\",\n",
    "    jac=True,\n",
    "    callback=cb.call\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Изобразите на графике кривую качества на обучени ии контроле по итерациям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "ax[0].plot(cb.train_acc, label=\"train acc\")\n",
    "ax[0].plot(cb.test_acc, label=\"test acc\")\n",
    "ax[0].set_xlabel(\"Iteration\")\n",
    "ax[0].set_ylabel(\"Accuracy\")\n",
    "ax[0].grid()\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(cb.train_ce, label=\"train ce\")\n",
    "ax[1].plot(cb.test_ce, label=\"test ce\")\n",
    "ax[1].set_xlabel(\"Iteration\")\n",
    "ax[1].set_ylabel(\"Cross Entropy\")\n",
    "ax[1].grid()\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Эксперименты с числом слоев (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Ясно, что из-за случайного начального приближения с каждым запуском обучения мы будем получать различное качество. Попробуем обучать нашу нейросеть с разным числом слоев несколько раз.\n",
    "\n",
    "Заполните матрицы accs_train и accs_test. В позиции [i, j] должна стоять величина точности сети с $i+1$ полносвязными слоями при $j$-м запуске (все запуски идентичны)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "accs_train = np.zeros((6, 5))\n",
    "accs_test = np.zeros((6, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "for idx in range(accs_train.shape[0]):\n",
    "    for jdx in range(accs_train.shape[1]):\n",
    "        network = make_network(32, idx + 1)\n",
    "        cb = Callback(network, X_train, y_train, X_test, y_test, print=False)\n",
    "        weights = get_weights(network)        \n",
    "        res = minimize(\n",
    "            compute_loss_grad, weights,  \n",
    "            args=[network, X_train, y_train], \n",
    "            method=\"L-BFGS-B\",\n",
    "            jac=True,\n",
    "            callback=cb.call\n",
    "        )\n",
    "        accs_train[idx, jdx] = cb.train_acc[-1]\n",
    "        accs_test[idx, jdx] = cb.test_acc[-1]         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Построим боксплоты полученного качества (горизонтальная линия в каждом столбце - среднее, прямоугольник показывает разброс)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "ax[0].boxplot(accs_train.T, showfliers=False)\n",
    "ax[0].set_xlabel(\"Num layers\")\n",
    "ax[0].set_ylabel(\"Train accuracy\")\n",
    "ax[0].set_title('Train quality in 5 runs')\n",
    "ax[0].set_ylim([0., 1.05])\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].boxplot(accs_test.T, showfliers=False)\n",
    "ax[1].set_xlabel(\"Num layers\")\n",
    "ax[1].set_ylabel(\"Test accuracy\")\n",
    "ax[1].set_title('Test quality in 5 runs')\n",
    "ax[1].set_ylim([0., 1.05])\n",
    "ax[1].grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Ответьте на вопросы (кратко в этой же ячейке):\n",
    "* Как изменяются качество на обучении и контроле и устойчивость процесса обучения при увеличении числа слоев?  \n",
    "** При небольшом числе слоёв (до пяти включительно) модель достигает идеальной точности классификации на обучающей выборке и имеет небольшую дисперсию качества на тестовой выборке. При этом качество незначительно меняется при изменении числа слоёв. Однако, при числе слоёв большем шести сеть перестаёт обучаться, что выражается в качестве классификации порядка 10% на обучающей и тестовой выборках. Это может быть связано с затуханием градиентов в глубоких сетях.\n",
    "* Можно ли сказать, что логистическая регрессия (линейная модель) дает качество хуже, чем нелинейная модель?  \n",
    "** В целом, логистическая регрессия (однослойная нейронная сеть) даёт сравнимое качество в данной задаче, хотя большее число слоёв позволяет улучшить качество на несколько процентов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "\\* Несколько фрагментов кода в задании написаны на основе материалов [курса по глубинному обучению на ФКН НИУ ВШЭ](https://www.hse.ru/ba/ami/courses/205504078.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Бонусная часть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Реализация метода оптимизации (1 балл)\n",
    "\n",
    "Реализуйте сами метод оптимизации (аналог функции minimize) для рассмотренной выше архитектуры. В качестве метода оптимизации используйте SGD + momentum. Продемонстрируйте правильную работу метода оптимизации, сравните его работы с LBFGS-B. Сделайте выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Dropout (1 балл) \n",
    "\n",
    "Реализуйте слой Dropout. Сравните обучение сети из большого числа слоёв при использовании Dropout и без его использования (предварительно подберите адекватный параметр p). Сделайте выводы. Используя метод оптимизации из первого бонусного пункта. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## BatchNormalization (1 балл)\n",
    "\n",
    "Реализуйте слой BatchNormalization. Сравните обучение сети из большого числа слоёв при использовании BatchNormalization и без его использования.  Сделайте выводы. Используя метод оптимизации из первого бонусного пункта. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (01.nn_numpy)",
   "language": "python",
   "name": "pycharm-ab619662"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
